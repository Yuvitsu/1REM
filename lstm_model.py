import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from load_data_label import DataLoader
from create_dataset import DataProcessor

# --- LSTM ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ ---
class LSTMModel(keras.Model):
    def __init__(self, num_units=128, num_layers=2, dropout_rate=0.2):
        """
        LSTM ãƒ™ãƒ¼ã‚¹ã®æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        Args:
            num_units (int): LSTM ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            num_layers (int): LSTM ã®å±¤æ•°
            dropout_rate (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(LSTMModel, self).__init__()

        self.lstm_layers = [
            layers.LSTM(num_units, return_sequences=True, dropout=dropout_rate) for _ in range(num_layers - 1)
        ]
        self.lstm_layers.append(layers.LSTM(num_units, return_sequences=False, dropout=dropout_rate))

        self.output_layer = layers.Dense(6, activation="linear")

    def call(self, inputs, training=False):
        x = inputs
        for lstm in self.lstm_layers:
            x = lstm(x, training=training)
        return self.output_layer(x)

# --- ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« ---
def build_lstm(input_shape):
    model = LSTMModel()
    model.build(input_shape=(None,) + input_shape)  # ğŸ”¹ ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ“ãƒ«ãƒ‰
    optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)
    model.compile(
        optimizer=optimizer,
        loss="mse",
        metrics=["mae"]
    )
    return model

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

    print("=== ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã‚’é–‹å§‹ ===")
    data_loader = DataLoader(data_dir="Data_Label/Gym")
    x_data, y_label = data_loader.load_data()

    print("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã‚’é–‹å§‹ ===")
    data_processor = DataProcessor(x_data, y_label, batch_size=64)
    train_dataset, val_dataset, test_dataset = data_processor.get_datasets()

    # ğŸ”¹ ã“ã“ã§ input_shape ã‚’å–å¾—
    sample_input_shape = x_data.shape[1:]  # ä¾‹: (10, 6)

    print("=== LSTM ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ ===")
    lstm_model = build_lstm(sample_input_shape)  # ğŸ”¹ ä¿®æ­£: å½¢çŠ¶ã‚’æ¸¡ã—ã¦ãƒ“ãƒ«ãƒ‰
    lstm_model.summary()  # âœ… ã“ã‚Œã§ã‚¨ãƒ©ãƒ¼ãªãå®Ÿè¡Œã§ãã‚‹

    print("=== ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ ===")
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, min_lr=1e-5)

    lstm_model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=20,
        callbacks=[early_stopping, lr_scheduler]
    )

    print("=== ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ ===")
    test_loss, test_mae = lstm_model.evaluate(test_dataset)
    print(f"Test Loss: {test_loss}, Test MAE: {test_mae}")

    print("=== ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ ===")
    lstm_model.save("lstm_model.keras")

    print("=== ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ†ã‚¹ãƒˆ ===")
    test_iter = iter(test_dataset)
    x_test_sample, y_test_sample = next(test_iter)

    predictions = lstm_model.predict(x_test_sample)

    print("Actual y_test:", y_test_sample.numpy()[:5])
    print("Predicted y:", predictions[:5])
